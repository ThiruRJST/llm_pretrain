#Reading the tamil characters from the text file
import os
from pathlib import Path

data_path = "dataset/tamil_pretrain.txt"


def chunk_filereader(fp, block_size:int = 8192):
    while True:
        data_chunk = fp.read(block_size)
        if not data_chunk:
            break
        yield data_chunk


def read_file_main(filepath: str, encoding_type: str):
    print(f"Reading the file: {filepath}")
    print(f"Size of the file: {os.path.getsize(filepath)}")
    print("**********************************************")
    print()
    with open(filepath, "r", encoding=encoding_type) as data_file:
        for chunk in chunk_filereader(data_file):
            print(chunk)


read_file_main(filepath=data_path, encoding_type="utf-8")


sample_text = "இந்தியா சார்பாக கடந்த போட்டியில் விளையாடிய உமேஷ் யாதவ்வைப் பிரதியிட்ட நவ்தீப் சைனி"


utf_8_encodes = list(sample_text.encode("utf-8"))


utf16_encodes = list(sample_text.encode("utf-16"))


utf32_encodes = list(sample_text.encode("utf-32"))


print(len(utf_8_encodes), len(utf16_encodes), len(utf32_encodes))


### Lets write the BPE Tokenizer from scratch
sample = sample_text
print("---")
print(sample)
print("---")
tokens = sample.encode("utf-8") # raw bytes
print(f"length of raw bytes: {len(tokens)}")
print(tokens)
print("---")
tokens = list(map(int, tokens)) # raw bytes converted to numbers
print(tokens)
print(f"length of raw bytes converted to numbers: {len(tokens)}")


def find_pairs_count(token_list: list):
    pair_dict = {}
    for i, ele in enumerate(token_list):
        if i < len(token_list)-1:
            pairs = (token_list[i], token_list[i+1])
            if pairs not in pair_dict:
                pair_dict[pairs] = 1
            else:
                pair_dict[pairs] += 1

    return pair_dict
            


pairwise_freq = find_pairs_count(tokens)


print(pairwise_freq)


print(sorted(((v, k) for k, v in pairwise_freq.items()), reverse=True))


bytes([224,174,135]).decode("utf-8")





import os
data_path = "dataset"
