{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "508747c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    pretrain_raw_data = \"dataset/tamil_pretrain.txt\"\n",
    "    context_length = 512\n",
    "    filtered_data = \"dataset/filtered_cl_tamil_pretrain.txt\"\n",
    "    vocab_size = 30000\n",
    "    tokenizer_path = \"artifacts/tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a344193",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84d10466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "import re\n",
    "\n",
    "def clean_whitespaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "\n",
    "def filter_sentences(file_path, max_len, output_file):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        mmapped_file = mmap.mmap(\n",
    "            f.fileno(),\n",
    "            0,\n",
    "            access=mmap.ACCESS_READ\n",
    "        )\n",
    "        with open(output_file, \"w\") as out_file:\n",
    "            for line in iter(mmapped_file.readline, b\"\"):\n",
    "                sentence = line.decode(\"utf-8\").strip()\n",
    "                sentence = clean_whitespaces(sentence)\n",
    "                if sentence:\n",
    "                    if len(sentence) >= max_len:\n",
    "                        out_file.write(sentence + \"\\n\")\n",
    "        mmapped_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c906db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sentences(Config.pretrain_raw_data, Config.context_length, \"dataset/filtered_cl_tamil_pretrain.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefc2e5",
   "metadata": {},
   "source": [
    "## Training a custom tokenizer\n",
    "This block helps to train a BPE tokenizer from scratch for the new filtered dataset.\n",
    "\n",
    "You can learn how to build a tokenizer from scratch block by block here: [Link](https://huggingface.co/learn/nlp-course/en/chapter6/8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "305fa77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a4863c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8628b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b46b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('├а┬о┬╡├а┬о┬г├а┬о─╖', (0, 3)),\n",
       " ('├а┬п─п', (3, 4)),\n",
       " ('├а┬о─╖├а┬о┬о', (4, 6)),\n",
       " ('├а┬п─п', (6, 7)),\n",
       " ('─а├а┬о─л├а┬о┬▓├а┬о─╖├а┬о┬о', (7, 12)),\n",
       " ('├а┬п─п', (12, 13))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tamil sample sentence\n",
    "sample_text = \"ро╡рогроХрпНроХроорпН роЙро▓роХроорпН\"\n",
    "tamil_tokenizer.pre_tokenizer.pre_tokenize_str(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5922cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=Config.vocab_size, special_tokens=[\"<|endoftext|>\"])\n",
    "tamil_tokenizer.model = models.BPE()\n",
    "tamil_tokenizer.train([Config.filtered_data], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6519ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec357081",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c65f3037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ро╡рогроХрпНроХроорпН роЙро▓роХроорпН'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamil_tokenizer.decode(\n",
    "    tamil_tokenizer.encode(\"ро╡рогроХрпНроХроорпН роЙро▓роХроорпН\").ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74356506",
   "metadata": {},
   "outputs": [],
   "source": [
    "tamil_tokenizer.save(Config.tokenizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a5b9e",
   "metadata": {},
   "source": [
    "## Training pipeline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c4ed1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ee6928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a54dd37d80764ec28c59a81c7d5c857d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaccdf691c74c6084eb63a7b11ed42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loading the splits\n",
    "data_files ={\n",
    "    \"train\": \"dataset/v2/tamil_train.txt\",\n",
    "    \"test\": \"dataset/v2/tamil_test.txt\",\n",
    "}\n",
    "\n",
    "raw_dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files=data_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6e3ffaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 3\n",
      "Input chunk lengths: [512, 390, 496]\n",
      "Chunk mapping: [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=Config.tokenizer_path,\n",
    ")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_dataset[\"train\"][:2]['text'],\n",
    "    truncation=True,\n",
    "    max_length=Config.context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True\n",
    "\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {outputs['length']}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4dba7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['роГрокро╛ропрпЖроЯрпНроЯрпЖ (1825) роЖроХро┐ропро╡ро▒рпНро▒ро╛ро▓рпН роЪро╛роорпБро╡рпЗро▓рпН роорпЛро░рпНро╕рпБроХрпНроХрпБ роЬройроиро╛ропроХ родрпЗроЪро┐роп роЙрогро░рпНро╡ро┐ройрпН роорпАродрпБ роХрпБро┤рокрпНрокроорпН роПро▒рпНрокроЯрпНроЯродрпБ. роЪрпЖропро▓рпН роорпБро▒рпИропро┐ро▓рпН роЕроорпЖро░ро┐роХрпНроХ роороХрпНроХро│ро╛роЯрпНроЪро┐ропрпИроХрпН роХро╛рогрпНрокро┐рокрпНрокродро▒рпНроХро╛роХ роЕроорпЖро░ро┐роХрпНроХ рокро┐ро░родро┐роиро┐родро┐роХро│рпН роЕро╡рпИропрпИ роЕро╡ро░рпН роУро╡ро┐ропрооро╛роХ ро╡ро░рпИропродрпН родрпЗро░рпНроирпНродрпЖроЯрпБродрпНродро╛ро░рпН. роЕро╡ро░рпН рокрпБродро┐роп роЕро▒рпИроХро│ро┐ройрпН роХроЯрпНроЯроорпИрокрпНрокрпИ ро╡ро░рпИро╡родро▒рпНроХрпБ ро╡ро╛ро╖ро┐роЩрпНроЯройрпН D.C.роХрпНроХрпБ рокропрогро┐родрпНродро╛ро░рпН. роЕроирпНрод роУро╡ро┐ропродрпНродро┐ро▓рпН роХро╡ройрооро╛роХ роОрогрпНрокродрпБ рокрпЗро░рпИ ро╡ро░рпИроирпНродро┐ро░рпБроирпНродро╛ро░рпН. роорпЗро▓рпБроорпН роЕродро┐ро▓рпН роЗроЯроорпН рокрпЖро▒рпНро▒ро┐ро░рпБроирпНрод роЗро░ро╡рпБроХрпНроХро╛роЯрпНроЪро┐ рокрпКро░рпБродрпНродрооро╛ройродро╛роХ роЗро░рпБрокрпНрокродро╛роХ роироорпНрокро┐ройро╛ро░рпН. роЕро╡ро░рпН ро╡роЯрпНроЯрооро╛рой роорогрпНроЯрокродрпНродро┐ройрпН ро╡ро┐ро│роХрпНроХрпБроХро│ро┐ройрпН ро╡рпЖро│ро┐роЪрпНроЪродрпНродро┐ройрпН роХрпБро╡ро┐родро┐ро▒ройрпБроХрпНроХрпБ роПро▒рпНрок роЕродро┐ро▓рпН роЗроЯроорпН рокрпЖро▒рпНро▒ро┐ро░рпБроирпНродрпЛро░рпИ ро╡рпЖро▒рпНро▒ро┐роХро░рооро╛роХ роЪрпАро░ро╛роХ ро╡ро░рпИроирпНродро┐ро░рпБроирпНродро╛ро░рпН. роЗрогрпИроирпНродро┐ро░рпБроХрпНроХрпБроорпН роороХрпНроХро│рпН, родройро┐ропро╛роХ роиро┐ройрпНро▒ро┐ро░рпБрокрпНрокрпЛро░рпН, родроЩрпНроХро│рпН роорпЗроЬрпИроХро│ро┐ройрпН роорпАродрпБ роЪро╛ропрпНроирпНродро┐ро░рпБроХрпНроХрпБроорпН родройро┐ропро╛ро│ро░рпН рокрпЛройрпНро▒ро╡рпИ роЪро╛родро╛ро░рогрооро╛роХ ро╡ро░рпИропрокрпНрокроЯрпНроЯро┐ро░рпБроирпНродродрпБ. роЖройро╛ро▓рпН роЪро┐ро▒рокрпНрокро┐ропро▓рпНрокрпБроЯройрпН роХрпВроЯро┐роп роорпБроХроЩрпНроХро│ро╛роХ роЗро░рпБроирпНродродрпБ. роЪро╛роорпБро╡рпЖро▓рпН роорпЛро░рпНро╕рпН роороХрпНроХро│ро╛роЯрпНроЪро┐ роХрпКро│рпНроХрпИроХро│ро┐ройрпН роорпАродрпБ роХро╛роЩрпНроХро┐ро░ро╕рпНроЪро┐ройрпН роЕро░рпНрокрпНрокрогро┐рокрпНрокрпБ роХро╛ро▓родрпНродро┐ройрпН роЕройрпБрокро╡ ро╡ро░роорпНрокрпИродрпН родро╛рогрпНроЯро┐ропродрпИ ро╡рпЖро│ро┐рокрпНрокроЯрпБродрпНродрпБро╡родро▒рпНроХрпБ роЗро░ро╡рпБ роирпЗро░родрпНродрпИродрпН родрпЗро░рпНроирпНродрпЖроЯрпБродрпНродро╛ро░рпН. роОройро┐ройрпБроорпН роХро╛роЩрпНроХро┐ро░ро╕рпН роиро┐ропрпВропро╛ро░рпНроХрпН роироХро░родрпНродро┐ро▓рпН роороХрпНроХро│рпН роХрпВроЯрпНроЯродрпНродрпИрокрпН рокрпЖро▒родрпН родро╡ро▒ро┐ропродрпБ. 1820 роЖроорпН роЖрогрпНроЯро┐ро▓рпН роЬро╛ройрпН роЯрпНро░роорпНрокрпБро▓ро┐ройрпН роЪрпБродроирпНродро┐ро░ роЕро▒ро┐ро╡ро┐рокрпНрокрпБроЪрпН роЪро╛ропрпИ роЕройрпИро╡ро░ро╛ро▓рпБроорпН роЖро░рпНрокрпНрокро░ро┐рокрпНрокрпБроЯройрпН роПро▒рпНро▒рпБроХрпНроХрпКро│рпНро│рокрпНрокроЯрпНроЯрпБ ро╡рпЖро▒рпНро▒ро┐ропроЯрпИроирпНродродрпБ. роПрооро╛ро▒рпНро▒родрпНродро┐ро▒рпНроХрпБ роЪро╛родрпНродро┐ропрооро╛рой роХро╛ро░рогроЩрпНроХро│ро┐ро▓рпН роТройрпНро▒ро╛роХ роЕроорпИроирпНродродрпБ. роТро░рпБро╡рпЗро│рпИ роЪро┐ро▓ро░рпН роЕроорпЖро░ро┐роХрпНроХ роЕро░роЪро┐ройрпН роЙро│рпН ро╡рпЗро▓рпИроХро│рпИ рокро╛ро░ро╛роЯрпНроЯро╛рооро▓рпН рокрпЛропро┐ро░рпБроХрпНроХро▓ро╛роорпН.',\n",
       " \"роородрпНродро┐роп роЕроорпИроЪрпНроЪро░ро╛рой ро░ро╛родро┐роХро╛ роЪрпЖро▓рпНро╡ро┐ропро┐роЯроорпН роЗродро▒рпНроХрпЖро▓рпНро▓ро╛роорпН ро╡ро┐ро│роХрпНроХроорпН роХрпЗроЯрпНроЯрокрпЛродрпБ, ''ропро╛ро░рпБ 'роУроЯрпИ' роЪрпЖро▓рпНро╡ройро╛ роЗродрпИропрпЖро▓рпНро▓ро╛роорпН роЙроЩрпНроХроХрпН роХро┐роЯрпНроЯ роЪрпКройрпНройродрпБ? роЕро╡ро░рпН, 'роЕроХро┐ро▓ роЗроирпНродро┐роп роиро╛роЯро╛ро░рпН рокро╛родрпБроХро╛рокрпНрокрпБрокрпН рокрпЗро░ро╡рпИ'ропро┐ро▓ роЗро░рпБроХрпНроХрпБ ро▒рпЗройрпНройрпБ роЪрпКро▓рпНро▓ро┐роХрпНроХро┐роЯрпНроЯрпБ роЕ.родро┐.роорпБ.роХ-роХро╛ро░роЩрпНроХ рокро┐ройрпНройро╛ро▓ роЪрпБродрпНродрпБро▒ро╡ро░ро╛роЪрпНроЪрпЗ. роЕро╡ро░рпБ родройрпНройрпИрокрпН рокрогрпНрогрпИропро╛ро░рпЛроЯ роЙрогрпНроорпИ ро╡ро┐роЪрпБро╡ро╛ роЪро┐ройрпНройрпБ роЪрпКро▓рпНро▓ро┐роХрпНроХро┐ро▒ро╛ро░ро╛? роЕро╡ро░рпБроорпЗро▓ рокрпЛро▓рпАро╕рпН ро╕рпНроЯрпЗро╖ройрпНроХро│ро┐ро▓рпН роОродрпНродройрпИ ро╡ро┤роХрпНроХрпБроХро│рпН рокродро┐ро╡ро╛роХро┐ роЗро░рпБроХрпНроХрпБройрпНройрпБ рокро╛ро░рпБроЩрпНроХ. роЕрокрпНрокрпБро▒роорпН, роЕро╡ро░рпБ роЪрпКро▓рпНро▒ роХро░рпБродрпНродрпИ роОроЯрпБродрпНродрпБроХрпНроХро▓ро╛роорпН. роПройрпН, роЪроЯрпНроЯроЪрокрпИ родрпЗро░рпНродро▓рпНро▓роХрпВроЯ роЕ.родро┐.роорпБ.роХ. роорпБройрпНройро╛ро│рпН роЕроорпИроЪрпНроЪро░рпН роЕройро┐родро╛ ро░ро╛родро╛роХро┐ро░рпБро╖рпНрогройрпБроХрпНроХрпБродро╛ройрпЗ роЕро╡ро░рпН ро╡рпЗро▓рпИ рокро╛ро░рпНродрпНродро╛ро░рпН! роЙрогрпНроорпИропрпИ роЪрпКро▓рпНро▓рогрпБроорпНройро╛ роЕро╡ро░рпН роЕройро┐родро╛ ро░ро╛родро╛роХро┐ро░рпБро╖рпНрогройрпЛроЯ роХрпИродрпНродроЯро┐. роЗрокрпНрокроХрпНроХрпВроЯ роЕ.родро┐.роорпБ.роХ-роХро╛ро░роЩрпНроХ родрпВрогрпНроЯрпБродро▓рпНро▓родро╛ройрпН роЕро╡ро░рпН роЗрокрпНрокроЯро┐ рокрпЗроЪрпБро▒ро╛ро░рпНройрпБ роиро┐ройрпИроХрпНроХро┐ро▒рпЗройрпН.\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][:2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5e9c5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d61c3bde8cd4085b61a5821d8997724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1184417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa3678829bb40d6a8e4025c91131342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345494 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=Config.context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True\n",
    "    )\n",
    "\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(\n",
    "        outputs['length'], outputs['input_ids']\n",
    "    ):\n",
    "        if length == Config.context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0fa81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1528958\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 445316\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfeba305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870c954bdce348c3bf97ae6ebd790354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=Config.context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "234dd62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 108.9M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 special tokens: {'pad_token': '[PAD]'}\n",
      "Pad token: '[PAD]', ID: 30000\n"
     ]
    }
   ],
   "source": [
    "# 1. First, add the pad token more explicitly\n",
    "if tokenizer.pad_token is None:\n",
    "    # Method 1: Set pad_token to eos_token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Method 2: If Method 1 doesn't work, try adding a special token\n",
    "    # This is more reliable as it modifies the tokenizer's vocabulary\n",
    "    special_tokens_dict = {'pad_token': '[PAD]'}\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    print(f\"Added {num_added_toks} special tokens: {special_tokens_dict}\")\n",
    "    \n",
    "    # If working with a model, resize embeddings to match new vocabulary size\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 2. Verify that pad token is set\n",
    "print(f\"Pad token: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f4675566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 512])\n",
      "attention_mask shape: torch.Size([5, 512])\n",
      "labels shape: torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "# 2. Create data collator with explicit padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False, \n",
    ")\n",
    "\n",
    "# 3. Try processing your batch\n",
    "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "434f9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tensorthiru/llm_pretrain/venv/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ЁЯдЧ Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_20927/3361315117.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"artifacts\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e74ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 139.06 MiB is free. Process 2120 has 102.55 MiB memory in use. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.92 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/trainer.py:2232\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 2232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2238\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2239\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3704\u001b[0m ):\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:441\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    439\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 441\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:369\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m    368\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> 369\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    371\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/transformers/activations.py:56\u001b[0m, in \u001b[0;36mNewGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3.0\u001b[39;49m\u001b[43m)\u001b[49m)))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.60 GiB of which 139.06 MiB is free. Process 2120 has 102.55 MiB memory in use. Including non-PyTorch memory, this process has 7.10 GiB memory in use. Of the allocated memory 6.92 GiB is allocated by PyTorch, and 55.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30fdbcc",
   "metadata": {},
   "source": [
    "## Transformer from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772b2eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0fc0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64 ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab98ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block:\n",
    "    # Masked Multi-head Attention\n",
    "    # Residual connection\n",
    "    # Layer Normalization\n",
    "    # Feed Forward\n",
    "    # Residual connection\n",
    "    # Layer Normalization\n",
    "\n",
    "#Output Head:\n",
    "    # Linear layer\n",
    "    # Softmax\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads: int, dim: int, dropout:float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.out_dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim = x.size()\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: t.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2), qkv)\n",
    "\n",
    "        #attention scores\n",
    "        attention = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
    "            attention.masked_fill_(mask[None, None, ...], float(\"-inf\"))\n",
    "        \n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        attention = self.attn_dropout(attention)\n",
    "\n",
    "        out = (attention @ v).transpose(1, 2).reshape(batch_size, seq_len, dim)\n",
    "        out = self.proj(out)\n",
    "        out = self.out_dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim: int = 512) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, dim * 4)\n",
    "        self.fc2 = nn.Linear(dim * 4, dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim:int=512, num_heads:int=8, dropout:float=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn_norm = nn.LayerNorm(dim)\n",
    "        self.attention = MultiHeadAttention(dim=dim, num_heads=num_heads, dropout=dropout)\n",
    "\n",
    "        self.ff = FeedForward(dim)\n",
    "        self.ff_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        residual = x\n",
    "        x = self.attn_norm(x)\n",
    "        x = self.attention(x, mask=mask)\n",
    "        x = residual + self.dropout(x)\n",
    "        \n",
    "        residual = x\n",
    "        x = self.ff_norm(x)\n",
    "        x = self.ff(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int = 2048):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create positional encodings\n",
    "        pe = torch.zeros(max_seq_len, dim)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2) * -(math.log(10000.0) / dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, dim]\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, num_tokens: int, dim: int, num_layers:int = 6):\n",
    "        super().__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.dim = dim\n",
    "        self.token_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.pos_emb = SinusoidalPositionalEncoding(dim=dim)\n",
    "        self.transformer = nn.ModuleList([Transformer(dim=dim) for _ in range(6)])\n",
    "        self.final_norm = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_tokens)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.pos_emb(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        x = self.final_norm(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8130337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [x for x in open(\"data.txt\", \"r\").readlines() if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ae2d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = []\n",
    "for line in data:\n",
    "    for char in line:\n",
    "        if char not in token_list:\n",
    "            token_list.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c277608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"\".join(sorted(token_list))\n",
    "num_tokens = len(tokens)\n",
    "char_to_token = {char: token for token, char in enumerate(tokens)}\n",
    "token_to_char = {token: char for token, char in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f94ceae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(num_tokens, 512).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8789c4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18981953 parameters in model\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters())} parameters in model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efdfcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the data using the map function\n",
    "def tokenize(line):\n",
    "    return torch.tensor([char_to_token[char] for char in line])\n",
    "\n",
    "\n",
    "def pad_truncate(tokens, target_length):\n",
    "    if len(tokens) > target_length:\n",
    "        return tokens[: target_length]\n",
    "    elif len(tokens) < target_length:\n",
    "        return F.pad(tokens, (0, target_length - len(tokens)))\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70e86aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "stk = torch.stack([pad_truncate(tokenize(line), 50) for line in data[:32]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac5f98b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd2a6c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(seed_token, model, context_length, max_new_tokens, temperature=1.0):\n",
    "    model.eval()\n",
    "    tokens = [char_to_token[seed_token]]\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Prepare input\n",
    "        x = torch.tensor(tokens[-context_length:]).unsqueeze(0).to(\"cuda:0\").long()\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = model(x)\n",
    "        \n",
    "        # Sample next token (from the last position)\n",
    "        logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        tokens.append(next_token)\n",
    "    \n",
    "    # Convert to text\n",
    "    return ''.join([token_list[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "959e1750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"&rqF.XIqHPWbA3YL?LURPjaSyiem!pu\\n\\nrRsU\\ncI;aGSNkVTf$-&wFuwha'zN LRV-:&sQEVXkcFMIT,irF mb?quSTnVrfof:JjJ\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"v\", model, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03226109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' ': 169892,\n",
       "         'e': 94611,\n",
       "         't': 67009,\n",
       "         'o': 65798,\n",
       "         'a': 55507,\n",
       "         'h': 51310,\n",
       "         's': 49696,\n",
       "         'r': 48889,\n",
       "         'n': 48529,\n",
       "         'i': 45537,\n",
       "         'l': 33339,\n",
       "         '\\n': 32777,\n",
       "         'd': 31358,\n",
       "         'u': 26584,\n",
       "         'm': 22243,\n",
       "         'y': 20448,\n",
       "         ',': 19846,\n",
       "         'w': 17585,\n",
       "         'f': 15770,\n",
       "         'c': 15623,\n",
       "         'g': 13356,\n",
       "         'I': 11832,\n",
       "         'b': 11321,\n",
       "         'p': 10808,\n",
       "         ':': 10316,\n",
       "         '.': 7885,\n",
       "         'A': 7819,\n",
       "         'v': 7793,\n",
       "         'k': 7088,\n",
       "         'T': 7015,\n",
       "         \"'\": 6187,\n",
       "         'E': 6041,\n",
       "         'O': 5481,\n",
       "         'N': 5079,\n",
       "         'R': 4869,\n",
       "         'S': 4523,\n",
       "         'L': 3876,\n",
       "         'C': 3820,\n",
       "         ';': 3628,\n",
       "         'W': 3530,\n",
       "         'U': 3313,\n",
       "         'H': 3068,\n",
       "         'M': 2840,\n",
       "         'B': 2761,\n",
       "         '?': 2462,\n",
       "         'G': 2399,\n",
       "         '!': 2172,\n",
       "         'D': 2089,\n",
       "         '-': 1897,\n",
       "         'F': 1797,\n",
       "         'Y': 1718,\n",
       "         'P': 1641,\n",
       "         'K': 1584,\n",
       "         'V': 798,\n",
       "         'j': 628,\n",
       "         'q': 609,\n",
       "         'x': 529,\n",
       "         'z': 356,\n",
       "         'J': 320,\n",
       "         'Q': 231,\n",
       "         'Z': 198,\n",
       "         'X': 112,\n",
       "         '3': 27,\n",
       "         '&': 3,\n",
       "         '$': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find number of times a character appears in the dataset\n",
    "from collections import Counter\n",
    "\n",
    "char_counter = Counter()\n",
    "for line in data:\n",
    "    char_counter.update(line)\n",
    "\n",
    "char_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd3b2626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1025 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0 Loss: 4.222636699676514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1025 [00:00<01:37, 10.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo&B.;;L&nlHP3y'IFlJsMZVlkORsepRq?YMihVBOQ:oYypo;usm' PEi;C;RBiA3UiGJO:FG Xup-nbJQCQpB:wr:K'yOlaO3-o\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|тЦИтЦП        | 124/1025 [00:05<00:39, 23.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 4000 Loss: 4.206521987915039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|тЦИтЦО        | 130/1025 [00:06<00:56, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&QrxeHsv:ljFpKbVqL\n",
      "WcC?tR,KcWinPwCaGyuz!U,:mgPrvamayIV&v?ooYjTJvtQJEBJ,OgGJMVQiz$VQgUTjEuTjp!k:,TjZV$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|тЦИтЦИтЦН       | 250/1025 [00:11<00:34, 22.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 8000 Loss: 4.227103233337402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|тЦИтЦИтЦН       | 253/1025 [00:11<00:54, 14.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!axv&ODvjJuCV$f-gt:eJ\n",
      "? taBVjZrJs\n",
      "&\n",
      "GURfD-gYKizsxuwiPetXH&?dYCcBlr:j ,niQV'zUVoF:rvy&V  ;yo\n",
      "aQKfJVuKX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|тЦИтЦИтЦИтЦЛ      | 373/1025 [00:17<00:28, 22.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 12000 Loss: 4.197488784790039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|тЦИтЦИтЦИтЦЛ      | 379/1025 [00:17<00:40, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbL!aTUKVVXl;eKl.MqcFr&IeDo&!?t:VFe.JtQ-s&?FVggNQSQU3' EVh?-iz-T?dW,BlSkZ:V!Z- uCFej$RCUZEpg.ipwd,lV?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|тЦИтЦИтЦИтЦИтЦК     | 499/1025 [00:22<00:23, 22.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 16000 Loss: 4.2015886306762695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|тЦИтЦИтЦИтЦИтЦЙ     | 505/1025 [00:23<00:32, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nOdS&FiRps-O,:kSDxAZJhtLIJV&s3j,UtdVX;Onhok Rigq,g$uLVQ-glJrg;ZDIttiBX.MfAkO\n",
      "wwC.opzEq&3KpRDr:FHxFGwF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|тЦИтЦИтЦИтЦИтЦИтЦИ    | 625/1025 [00:28<00:17, 22.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 20000 Loss: 4.213192939758301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|тЦИтЦИтЦИтЦИтЦИтЦИтЦП   | 628/1025 [00:29<00:28, 14.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wCj l;hUkE'DC\n",
      "LZj.3-v$CNCTJA&ieiv.s\n",
      "os?XvPl;$:MIJsRontVlVKOd3sZrw&ydYo::TpogmQ?rs$C\n",
      "$$mUqf;CvYJKv:Q!A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦО  | 748/1025 [00:34<00:12, 22.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 24000 Loss: 4.232711315155029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦО  | 754/1025 [00:35<00:17, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoB-LMdvagS fiHdWwFzYazz\n",
      "zj3kozPdRU'w$sCi\n",
      "P:HdzQ:wVym;KNUhqJWFJkLFy;zC'MKla'$OQKecTZDoV!,Ndm.S&-Y!gQ;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦМ | 874/1025 [00:40<00:06, 22.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 28000 Loss: 4.236649513244629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦМ | 880/1025 [00:40<00:09, 15.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFiBQr3ZJH:YjdlizaWF:NgQPrkLEd,Yb;ig,:mmHDvOv'dudQ.pad$DmOkQr VG\n",
      "JMFYBOj!,:,yMxbTEKPda!.bJMAPiK$V!Gm$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦК| 1000/1025 [00:46<00:01, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 32000 Loss: 4.229125499725342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦК| 1003/1025 [00:46<00:01, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "\n",
      "YiPRpe-WFSk&QAbeDZ;vyoRNJMD&bC-?Utb-o&iv.E,wsgv ia$EoJO:Yb3EF-hfqnp-OTlaWitlO&-YNDzCfJD&zwks&dULCj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|тЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИтЦИ| 1025/1025 [00:47<00:00, 21.44it/s]\n",
      "  0%|          | 0/1025 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 0 Loss: 4.220936298370361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1025 [00:00<01:39, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usVO Cgj?J&C;$cqdww3dow'Nl.umB gILPxbl;KOLpZu$\n",
      "jux.D?r:Q;kJOMKdK\n",
      "vhVuH$iDJW E?uG-DlkC;gdJJRsmT BiQtGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|тЦИтЦП        | 124/1025 [00:06<00:41, 21.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 4000 Loss: 4.194313049316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|тЦИтЦО        | 130/1025 [00:06<00:58, 15.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fa\n",
      "uope\n",
      "Y;'!wsAVhy!xk$yRKziYZcEJYiH!ONf-jVD!wsixBopmCuLQ?c-MoBVMJa$XJuCvedaJCJuCgOHkGJ-ksRxaDFyyqLB!U\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|тЦИтЦИтЦН       | 250/1025 [00:12<00:36, 21.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 8000 Loss: 4.230233192443848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|тЦИтЦИтЦН       | 253/1025 [00:12<00:57, 13.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XqoPHkDM,LjN$kzXb!xZe!LJ!b;uRQDQUDnu$MN\n",
      "i!'?qLi,UtunJu;&iA-HdZieKJ,:$Qv izBli$oJex'!HE-iet;ivF!SoqfJM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|тЦИтЦИтЦИтЦЛ      | 373/1025 [00:18<00:30, 21.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 12000 Loss: 4.204566955566406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|тЦИтЦИтЦИтЦЛ      | 379/1025 [00:18<00:42, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cUiidXNlHYQLASBkMkDz\n",
      "\n",
      ";OzJ\n",
      "?XJQBC.bQ'I\n",
      "qB-pVP\n",
      ",A\n",
      "wYZuU sEZJeP.vQYVuaUYGHM DUUwYhwF$yt$TKE,J;G&&btl.CX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|тЦИтЦИтЦИтЦИтЦК     | 499/1025 [00:24<00:24, 21.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 16000 Loss: 4.207834720611572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|тЦИтЦИтЦИтЦИтЦЙ     | 505/1025 [00:24<00:34, 15.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":VfnXaieg.ztYmsc&UWWrVhpeaJ.CG?wiWKtie\n",
      "h'vvxkQ.TTJQrRdEJjcdVi$xbei:NaIEwVQ-&\n",
      "h rbOikd&JC$b,!Zycjisnfk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|тЦИтЦИтЦИтЦИтЦИ     | 516/1025 [00:25<00:25, 20.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 25\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_tokens)\n\u001b[1;32m     27\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 125\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer:\n\u001b[0;32m--> 125\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[1;32m    127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 83\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     82\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_norm(x)\n\u001b[0;32m---> 83\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[17], line 57\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/llm_pretrain/venv/lib/python3.10/site-packages/torch/nn/functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "lr = 1e-6\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "context = 65\n",
    "\n",
    "\n",
    "\n",
    "model = GPT(num_tokens, 512).to(\"cuda\")\n",
    "\n",
    "for epoch in range(2):\n",
    "    step_loss = 0.0\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        x = data[i: i+batch_size]\n",
    "        x = torch.stack([pad_truncate(tokenize(line), context) for line in x])\n",
    "        y = x[:, 1:]\n",
    "        x = x[:, :-1]\n",
    "        x = x.to(\"cuda:0\").long()\n",
    "        y = y.to(\"cuda:0\").long()\n",
    "\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.view(-1, num_tokens)\n",
    "        y = y.view(-1)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        step_loss += loss.item()\n",
    "        if i % 1000 == 0:\n",
    "            model.eval()\n",
    "            print(f\"Epoch {epoch} Iteration {i} Loss: {loss.item()}\")\n",
    "            with torch.no_grad():\n",
    "                model_gen = generate(np.random.choice(token_list), model, 65, 100)\n",
    "            print(model_gen)  \n",
    "            model.train()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b58bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
